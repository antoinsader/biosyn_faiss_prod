{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\antoi\\Desktop\\thesis\\biosyn_faiss_prod\\myenc\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from config import GlobalConfig, tokenizer_parse_args\n",
    "from helpers.Data import TokensPaths, load_queries, load_dictionary\n",
    "from transformers import AutoTokenizer\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = GlobalConfig()\n",
    "dictionary_max_length = cfg.tokenize.dictionary_max_length\n",
    "dictionary_max_chars_length = cfg.tokenize.dictionary_max_chars_length\n",
    "queries_max_length = cfg.tokenize.queries_max_length\n",
    "mention_start_special_token = cfg.tokenize.special_tokens_dict[\"mention_start\"]\n",
    "mention_end_special_token = cfg.tokenize.special_tokens_dict[\"mention_end\"]\n",
    "tokens_paths  = TokensPaths(cfg, dictionary_key='dictionary', queries_key='train_queries')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.model.model_name, use_fast=True)\n",
    "tokenizer.add_special_tokens(cfg.tokenize.special_tokens)\n",
    "\n",
    "mention_start_token_id  = tokenizer.convert_tokens_to_ids(cfg.tokenize.special_tokens_dict[\"mention_start\"])\n",
    "mention_end_token_id  = tokenizer.convert_tokens_to_ids(cfg.tokenize.special_tokens_dict[\"mention_end\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 691/691 [00:01<00:00, 360.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotation_skipped: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_queries = load_queries(\n",
    "    data_dir=cfg.paths.queries_raw_dir,\n",
    "    queries_max_length=queries_max_length,\n",
    "    special_token_start=mention_start_special_token ,\n",
    "    special_token_end=mention_end_special_token,\n",
    "    tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizer import tokenize_names\n",
    "from datasets import Dataset\n",
    "import os\n",
    "\n",
    "max_length = cfg.tokenize.queries_max_length\n",
    "queries_names = [q[0] for q in train_queries]\n",
    "queries_cuis = [q[1] for q in train_queries]\n",
    "queries_sentences = [q[2] for q in train_queries]\n",
    "\n",
    "N = len(queries_sentences)\n",
    "tokenized = tokenizer(queries_sentences, padding=\"max_length\", truncation=True, max_length=max_length)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "inps_all  = torch.as_tensor(tokenized[\"input_ids\"])\n",
    "atts_all  = torch.as_tensor(tokenized[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 4599\n",
    "st = str(train_queries[idx][2])\n",
    "len(tokenizer(st)[\"input_ids\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized[\"input_ids\"][4599])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "mention_start_token_id  = tokenizer.convert_tokens_to_ids(cfg.tokenize.special_tokens_dict[\"mention_start\"])\n",
    "mention_end_token_id  = tokenizer.convert_tokens_to_ids(cfg.tokenize.special_tokens_dict[\"mention_end\"])\n",
    "inps_all = dataset.queries_input_ids\n",
    "for idx, inp  in enumerate(inps_all):\n",
    "    mention_start_position = (inp == mention_start_token_id).nonzero()[0], f\"query_idx: {idx}, does not have start: inps: {inp}\"\n",
    "    assert len(mention_start_position) > 0\n",
    "    mention_end_position = (inp == mention_end_token_id).nonzero()[0], f\"query_idx: {idx}, does not have end: inps: {inp}\"\n",
    "    assert len(mention_end_position) > 0\n",
    "\n",
    "mention_end_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(e[\"text\"], padding=\"max_length\", truncation=True, max_length=75)\n",
    "mention_start_positions = (inp == mention_start_token_id).nonzero()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "(tokens_size, max_length ) = tokens_paths.queries_shape\n",
    "batch_size = 128\n",
    "N = tokens_size\n",
    "\n",
    "query_inputs = dataset.queries_input_ids\n",
    "query_att = dataset.queries_attention_mask\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "start = 14719\n",
    "end = 14849\n",
    "cursor = 0\n",
    "\n",
    "for i in range(start, end):\n",
    "    print(f\"idx: {i}\")\n",
    "    mention_start_positions = (query_inputs[i] == mention_start_token_id).nonzero()[0]\n",
    "    mention_end_positions = (query_inputs[i] == mention_end_token_id).nonzero()[0]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(tokens_size, max_length ) = tokens_paths.queries_shape\n",
    "N = tokens_size\n",
    "\n",
    "query_inputs = dataset.queries_input_ids\n",
    "query_att = dataset.queries_attention_mask\n",
    "\n",
    "\n",
    "\n",
    "for start in range(0, N,batch_size):\n",
    "    end = min(start + batch_size, N)\n",
    "    print(f\"start:end: {start}-{end}\")\n",
    "    inp  = torch.as_tensor(query_inputs[start:end], device=device)\n",
    "    att = torch.as_tensor(query_att[start:end],device=device)\n",
    "    embs = my_encoder.get_emb(inp, att, use_amp=True, use_no_grad=True)\n",
    "    \n",
    "    del inp, att, embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_queries[14848]\n",
    "\n",
    "query_inputs[14848]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(tokens_size, max_length ) = tokens_paths.queries_shape\n",
    "N = tokens_size\n",
    "\n",
    "query_inputs = dataset.queries_input_ids\n",
    "query_att = dataset.queries_attention_mask\n",
    "\n",
    "start = 14819\n",
    "end = 14821\n",
    "print(f\"start:end: {start}-{end}\")\n",
    "inp  = torch.as_tensor(query_inputs[start:end], device=device)\n",
    "att = torch.as_tensor(query_att[start:end],device=device)\n",
    "embs = my_encoder.get_emb(inp, att, use_amp=True, use_no_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "14820\n",
    "\n",
    "tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cui = 'C1257890'\n",
    "queries = []\n",
    "for q in train_queries:\n",
    "    if q[1] == cui and q[2].find(\"[ME]\") == -1:\n",
    "        enc = tokenizer(q[2], padding=\"max_length\", truncation=True, max_length=75)\n",
    "        mention_end_positions = (query_inputs[i] == mention_end_token_id).nonzero()[0]\n",
    "        queries.append(q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, q in enumerate(queries):\n",
    "    enc = tokenizer(q[2], padding=\"max_length\", truncation=True, max_length=75)\n",
    "    print(f\"idx in queries: {idx}\")\n",
    "    k = (torch.as_tensor(enc[\"input_ids\"]) ==  mention_end_token_id).nonzero()[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tokenizer(s, padding=\"max_length\", truncation=True, max_length=75)\n",
    "tokens = tokenizer.convert_ids_to_tokens(enc[\"input_ids\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[MS]'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mention = 'populations'\n",
    "cui = 'C1257890'\n",
    "\n",
    "text = 'Epidemiology and cardiovascular comorbidities in patients with psoriasis: A Korean nationwide population-based cohort study There is a lack of nationwide studies examining the epidemiology and comorbidities of psoriasis vulgaris (PsV) and psoriatic arthritis (PsA) in Asian populations. The purpose of this study is to determine the demographics of psoriasis in Korea along with the incidence of cerebro - cardiovascular (CV) comorbidities and to compare these risks between populations with PsA and with PsV. This cohort study identified 15 484 patients with psoriasis among 855 003 subjects in the Korean National Health Insurance Database from 2002 through 2010. The cases were further classified into PsA and PsV. We used hazard ratios (HR) and 95% confidence intervals (CI) from the univariate and age - sex adjusted logistic regression model to assess the risk of comorbidities in patients with PsA and PsV. The annual prevalence of psoriasis increased from 313.2 to 453.5/100 000 people from 2002 through 2010; however, the overall incidence rate for psoriasis slightly decreased (252.7-212.6/100 000 population). Of psoriatic patients, 10.8% had PsA, and after adjusting for age and sex, PsA patients had a significantly higher risk of dyslipidemia than PsV patients (adjusted HR, 1.185; 95% CI, 1.049-1.338). When stratified by age group, subjects aged 20-39 years had a higher risk of stroke and many CV risk factors. In conclusion, the prevalence of psoriasis, while within the range of previous reports, tended to increase over time. Patients with PsA had higher burdens of specific comorbid diseases than those with PsV, especially at a comparatively early age.'\n",
    "\n",
    "\n",
    "mention_start = 274\n",
    "mention_end = 285\n",
    "\n",
    "\n",
    "\n",
    "special_token_start  = cfg.tokenize.special_tokens_dict[\"mention_start\"]\n",
    "special_token_end  = cfg.tokenize.special_tokens_dict[\"mention_end\"]\n",
    "\n",
    "text[mention_start:mention_end]\n",
    "special_token_start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tokenizer(annotated, padding=\"max_length\", truncation=True, max_length=75)\n",
    "enc\n",
    "mention_end_positions = (torch.as_tensor(enc[\"input_ids\"]) == mention_end_token_id).nonzero()[0]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
