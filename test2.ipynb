{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\antoi\\Desktop\\thesis\\biosyn_faiss_prod\\myenc\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from config import GlobalConfig, tokenizer_parse_args\n",
    "from helpers.Data import TokensPaths, load_queries, load_dictionary\n",
    "from transformers import AutoTokenizer\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = GlobalConfig()\n",
    "dictionary_max_length = cfg.tokenize.dictionary_max_length\n",
    "dictionary_max_chars_length = cfg.tokenize.dictionary_max_chars_length\n",
    "queries_max_length = cfg.tokenize.queries_max_length\n",
    "mention_start_special_token = cfg.tokenize.special_tokens_dict[\"mention_start\"]\n",
    "mention_end_special_token = cfg.tokenize.special_tokens_dict[\"mention_end\"]\n",
    "tokens_paths  = TokensPaths(cfg, dictionary_key='dictionary', queries_key='train_queries')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.model.model_name, use_fast=True)\n",
    "tokenizer.add_special_tokens(cfg.tokenize.special_tokens)\n",
    "\n",
    "mention_start_token_id  = tokenizer.convert_tokens_to_ids(cfg.tokenize.special_tokens_dict[\"mention_start\"])\n",
    "mention_end_token_id  = tokenizer.convert_tokens_to_ids(cfg.tokenize.special_tokens_dict[\"mention_end\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 691/691 [00:01<00:00, 360.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotation_skipped: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_queries = load_queries(\n",
    "    data_dir=cfg.paths.queries_raw_dir,\n",
    "    queries_max_length=queries_max_length,\n",
    "    special_token_start=mention_start_special_token ,\n",
    "    special_token_end=mention_end_special_token,\n",
    "    tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizer import tokenize_names\n",
    "from datasets import Dataset\n",
    "import os\n",
    "\n",
    "max_length = cfg.tokenize.queries_max_length\n",
    "queries_names = [q[0] for q in train_queries]\n",
    "queries_cuis = [q[1] for q in train_queries]\n",
    "queries_sentences = [q[2] for q in train_queries]\n",
    "\n",
    "N = len(queries_sentences)\n",
    "tokenized = tokenizer(queries_sentences, padding=\"max_length\", truncation=True, max_length=max_length)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "inps_all  = torch.as_tensor(tokenized[\"input_ids\"])\n",
    "atts_all  = torch.as_tensor(tokenized[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 4599\n",
    "st = str(train_queries[idx][2])\n",
    "len(tokenizer(st)[\"input_ids\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized[\"input_ids\"][4599])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "mention_start_token_id  = tokenizer.convert_tokens_to_ids(cfg.tokenize.special_tokens_dict[\"mention_start\"])\n",
    "mention_end_token_id  = tokenizer.convert_tokens_to_ids(cfg.tokenize.special_tokens_dict[\"mention_end\"])\n",
    "inps_all = dataset.queries_input_ids\n",
    "for idx, inp  in enumerate(inps_all):\n",
    "    mention_start_position = (inp == mention_start_token_id).nonzero()[0], f\"query_idx: {idx}, does not have start: inps: {inp}\"\n",
    "    assert len(mention_start_position) > 0\n",
    "    mention_end_position = (inp == mention_end_token_id).nonzero()[0], f\"query_idx: {idx}, does not have end: inps: {inp}\"\n",
    "    assert len(mention_end_position) > 0\n",
    "\n",
    "mention_end_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(e[\"text\"], padding=\"max_length\", truncation=True, max_length=75)\n",
    "mention_start_positions = (inp == mention_start_token_id).nonzero()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "(tokens_size, max_length ) = tokens_paths.queries_shape\n",
    "batch_size = 128\n",
    "N = tokens_size\n",
    "\n",
    "query_inputs = dataset.queries_input_ids\n",
    "query_att = dataset.queries_attention_mask\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "start = 14719\n",
    "end = 14849\n",
    "cursor = 0\n",
    "\n",
    "for i in range(start, end):\n",
    "    print(f\"idx: {i}\")\n",
    "    mention_start_positions = (query_inputs[i] == mention_start_token_id).nonzero()[0]\n",
    "    mention_end_positions = (query_inputs[i] == mention_end_token_id).nonzero()[0]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(tokens_size, max_length ) = tokens_paths.queries_shape\n",
    "N = tokens_size\n",
    "\n",
    "query_inputs = dataset.queries_input_ids\n",
    "query_att = dataset.queries_attention_mask\n",
    "\n",
    "\n",
    "\n",
    "for start in range(0, N,batch_size):\n",
    "    end = min(start + batch_size, N)\n",
    "    print(f\"start:end: {start}-{end}\")\n",
    "    inp  = torch.as_tensor(query_inputs[start:end], device=device)\n",
    "    att = torch.as_tensor(query_att[start:end],device=device)\n",
    "    embs = my_encoder.get_emb(inp, att, use_amp=True, use_no_grad=True)\n",
    "    \n",
    "    del inp, att, embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_queries[14848]\n",
    "\n",
    "query_inputs[14848]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(tokens_size, max_length ) = tokens_paths.queries_shape\n",
    "N = tokens_size\n",
    "\n",
    "query_inputs = dataset.queries_input_ids\n",
    "query_att = dataset.queries_attention_mask\n",
    "\n",
    "start = 14819\n",
    "end = 14821\n",
    "print(f\"start:end: {start}-{end}\")\n",
    "inp  = torch.as_tensor(query_inputs[start:end], device=device)\n",
    "att = torch.as_tensor(query_att[start:end],device=device)\n",
    "embs = my_encoder.get_emb(inp, att, use_amp=True, use_no_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "14820\n",
    "tokens = tokenizer.convert_ids_to_tokens(query_inputs[14820])\n",
    "\n",
    "tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cui = 'C1257890'\n",
    "queries = []\n",
    "for q in train_queries:\n",
    "    if q[1] == cui and q[2].find(\"[ME]\") == -1:\n",
    "        enc = tokenizer(q[2], padding=\"max_length\", truncation=True, max_length=75)\n",
    "        mention_end_positions = (query_inputs[i] == mention_end_token_id).nonzero()[0]\n",
    "        queries.append(q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, q in enumerate(queries):\n",
    "    enc = tokenizer(q[2], padding=\"max_length\", truncation=True, max_length=75)\n",
    "    print(f\"idx in queries: {idx}\")\n",
    "    (torch.as_tensor(enc[\"input_ids\"]) ==  mention_end_token_id).nonzero()[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " 'iovascular com orbid', '##ities', 'in', 'patients', 'with', 'ps', '##oria', '##sis', ':', 'a', 'k', '##ore', '##an', 'nationwide', 'population', '-', 'based', 'co', '##hor', '##t', 'study', 'there', 'is', 'a', 'lack', 'of', 'nationwide', 'studies', 'examining', 'the', 'e', '##pid', '##em', '##iology', 'and', 'com', '##or', '##bid', '##ities', 'of', 'ps', '##oria', '##sis', 'v', '##ul', '##gar', '##is', '(', 'ps', '##v', ')', 'and', 'ps', '##oria', '##tic', 'art', '##hr', '##itis', '(', 'ps', '##a', ')', 'in', 'as', '##ian', '[MS]', 'populations', '[SEP]']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
